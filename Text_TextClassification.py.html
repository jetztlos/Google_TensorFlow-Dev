<html>
<head>
<title>Text_TextClassification.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #8c8c8c; font-style: italic;}
.s1 { color: #080808;}
.s2 { color: #0033b3;}
.s3 { color: #067d17;}
.s4 { color: #1750eb;}
</style>
</head>
<body bgcolor="#ffffff">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#c0c0c0" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
Text_TextClassification.py</font>
</center></td></tr></table>
<pre><span class="s0"># From: https://www.tensorflow.org/text/tutorials/text_classification_rnn</span>
<span class="s0">## Setup</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">tensorflow_datasets </span><span class="s2">as </span><span class="s1">tfds</span>
<span class="s2">import </span><span class="s1">tensorflow </span><span class="s2">as </span><span class="s1">tf</span>

<span class="s1">tfds.disable_progress_bar()</span>

<span class="s2">import </span><span class="s1">matplotlib.pyplot </span><span class="s2">as </span><span class="s1">plt</span>

<span class="s2">def </span><span class="s1">plot_graphs(history, metric):</span>
    <span class="s1">plt.plot(history.history[metric])</span>
    <span class="s1">plt.plot(history.history[</span><span class="s3">'val_'</span><span class="s1">+metric], </span><span class="s3">''</span><span class="s1">)</span>
    <span class="s1">plt.xlabel(</span><span class="s3">&quot;Epochs&quot;</span><span class="s1">)</span>
    <span class="s1">plt.ylabel(metric)</span>
    <span class="s1">plt.legend([metric, </span><span class="s3">'val_'</span><span class="s1">+metric])</span>

<span class="s0">## Setup input pipeline</span>
<span class="s1">dataset, info = tfds.load(</span><span class="s3">'imdb_reviews'</span><span class="s1">, with_info=</span><span class="s2">True</span><span class="s1">, as_supervised=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s1">train_dataset, test_dataset = dataset[</span><span class="s3">'train'</span><span class="s1">], dataset[</span><span class="s3">'test'</span><span class="s1">]</span>

<span class="s1">train_dataset.element_spec</span>

<span class="s2">for </span><span class="s1">example, label </span><span class="s2">in </span><span class="s1">train_dataset.take(</span><span class="s4">1</span><span class="s1">):</span>
    <span class="s1">print(</span><span class="s3">'text: '</span><span class="s1">, example.numpy())</span>
    <span class="s1">print(</span><span class="s3">'label: '</span><span class="s1">, label.numpy())</span>

<span class="s1">BUFFER_SIZE = </span><span class="s4">10_000</span>
<span class="s1">BATCH_SIZE = </span><span class="s4">64</span>

<span class="s1">train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)</span>
<span class="s1">test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)</span>

<span class="s2">for </span><span class="s1">example, label </span><span class="s2">in </span><span class="s1">train_dataset.take(</span><span class="s4">1</span><span class="s1">):</span>
    <span class="s1">print(</span><span class="s3">'texts: '</span><span class="s1">, example.numpy()[:</span><span class="s4">3</span><span class="s1">])</span>
    <span class="s1">print()</span>
    <span class="s1">print(</span><span class="s3">'labels: '</span><span class="s1">, label.numpy()[:</span><span class="s4">3</span><span class="s1">])</span>

<span class="s0">## Create the text encoder</span>
<span class="s1">VOCAB_SIZE = </span><span class="s4">1_000</span>
<span class="s1">encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)</span>
<span class="s1">encoder.adapt(train_dataset.map(</span><span class="s2">lambda </span><span class="s1">text, label: text))</span>

<span class="s1">vocab = np.array(encoder.get_vocabulary())</span>
<span class="s1">vocab[:</span><span class="s4">20</span><span class="s1">]</span>

<span class="s1">encoded_example = encoder(example)[:</span><span class="s4">3</span><span class="s1">].numpy()</span>
<span class="s1">encoded_example</span>

<span class="s2">for </span><span class="s1">n </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">3</span><span class="s1">):</span>
    <span class="s1">print(</span><span class="s3">&quot;Original: &quot;</span><span class="s1">, example[n].numpy())</span>
    <span class="s1">print(</span><span class="s3">&quot;Round-trip: &quot;</span><span class="s1">, </span><span class="s3">&quot; &quot;</span><span class="s1">.join(vocab[encoded_example[n]]))</span>
    <span class="s1">print()</span>

<span class="s0">## Create the model</span>
<span class="s1">model = tf.keras.Sequential([</span>
    <span class="s1">encoder,</span>
    <span class="s1">tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()), output_dim=</span><span class="s4">64</span><span class="s1">,</span>
                              <span class="s1">mask_zero=</span><span class="s2">True</span><span class="s1">), </span><span class="s0"># Use masking to handle the variable sequence lengths</span>
    <span class="s1">tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(</span><span class="s4">64</span><span class="s1">)),</span>
    <span class="s1">tf.keras.layers.Dense(</span><span class="s4">64</span><span class="s1">, activation=</span><span class="s3">'relu'</span><span class="s1">),</span>
    <span class="s1">tf.keras.layers.Dense(</span><span class="s4">1</span><span class="s1">)</span>
<span class="s1">])</span>

<span class="s1">print([layer.supports_masking </span><span class="s2">for </span><span class="s1">layer </span><span class="s2">in </span><span class="s1">model.layers])</span>

<span class="s0"># Predict on a sample text w/o padding.</span>
<span class="s1">sample_text = (</span><span class="s3">'The movie was cool. The animation and the graphics '</span>
               <span class="s3">'were out of this world. I would recommend this movie.'</span><span class="s1">)</span>
<span class="s1">predictions = model.predict(np.array([sample_text]))</span>
<span class="s1">print(predictions[</span><span class="s4">0</span><span class="s1">])</span>

<span class="s0"># Predict on a sample text w/ padding.</span>
<span class="s1">padding = </span><span class="s3">&quot;the &quot; </span><span class="s1">* </span><span class="s4">2_000</span>
<span class="s1">predictions = model.predict(np.array([sample_text, padding]))</span>
<span class="s1">print(predictions[</span><span class="s4">0</span><span class="s1">])</span>

<span class="s1">model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=</span><span class="s2">True</span><span class="s1">),</span>
              <span class="s1">optimizer=tf.keras.optimizers.Adam(</span><span class="s4">1e-4</span><span class="s1">),</span>
              <span class="s1">metrics=[</span><span class="s3">'accuracy'</span><span class="s1">])</span>

<span class="s0">## Train the model</span>
<span class="s1">history = model.fit(train_dataset,</span>
                    <span class="s1">epochs=</span><span class="s4">10</span><span class="s1">,</span>
                    <span class="s1">validation_data=test_dataset,</span>
                    <span class="s1">validation_steps=</span><span class="s4">30</span><span class="s1">)</span>

<span class="s1">test_loss, test_acc = model.evaluate(test_dataset)</span>

<span class="s1">print(</span><span class="s3">'Test Loss:'</span><span class="s1">, test_loss)</span>
<span class="s1">print(</span><span class="s3">'Test Accuracy:'</span><span class="s1">, test_acc)</span>

<span class="s1">plt.figure(figsize=(</span><span class="s4">16</span><span class="s1">, </span><span class="s4">8</span><span class="s1">))</span>
<span class="s1">plt.subplot(</span><span class="s4">1</span><span class="s1">, </span><span class="s4">2</span><span class="s1">, </span><span class="s4">1</span><span class="s1">)</span>
<span class="s1">plot_graphs(history, </span><span class="s3">'accuracy'</span><span class="s1">)</span>
<span class="s1">plt.ylim(</span><span class="s2">None</span><span class="s1">, </span><span class="s4">1</span><span class="s1">)</span>
<span class="s1">plt.subplot(</span><span class="s4">1</span><span class="s1">, </span><span class="s4">2</span><span class="s1">, </span><span class="s4">2</span><span class="s1">)</span>
<span class="s1">plot_graphs(history, </span><span class="s3">'loss'</span><span class="s1">)</span>
<span class="s1">plt.ylim(</span><span class="s4">0</span><span class="s1">, </span><span class="s2">None</span><span class="s1">)</span>

<span class="s1">sample_text = (</span><span class="s3">'The movie was cool. The animation and the graphics '</span>
               <span class="s3">'were out of this world. I would recommend this movie.'</span><span class="s1">)</span>
<span class="s1">predictions = model.predict(np.array([sample_text]))</span>

<span class="s0">## Stack 2 / more LSTM layers</span>
<span class="s1">model = tf.keras.Sequential([</span>
    <span class="s1">encoder,</span>
    <span class="s1">tf.keras.layers.Embedding(len(encoder.get_vocabulary()), </span><span class="s4">64</span><span class="s1">, mask_zero=</span><span class="s2">True</span><span class="s1">),</span>
    <span class="s1">tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(</span><span class="s4">64</span><span class="s1">, return_sequences=</span><span class="s2">True</span><span class="s1">)),</span>
    <span class="s1">tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(</span><span class="s4">32</span><span class="s1">)),</span>
    <span class="s1">tf.keras.layers.Dense(</span><span class="s4">64</span><span class="s1">, activation=</span><span class="s3">'relu'</span><span class="s1">),</span>
    <span class="s1">tf.keras.layers.Dropout(</span><span class="s4">0.5</span><span class="s1">),</span>
    <span class="s1">tf.keras.layers.Dense(</span><span class="s4">1</span><span class="s1">)</span>
<span class="s1">])</span>

<span class="s1">model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=</span><span class="s2">True</span><span class="s1">),</span>
              <span class="s1">optimizer=tf.keras.optimizers.Adam(</span><span class="s4">1e-4</span><span class="s1">),</span>
              <span class="s1">metrics=[</span><span class="s3">'accuracy'</span><span class="s1">])</span>

<span class="s1">history = model.fit(train_dataset,</span>
                    <span class="s1">epochs=</span><span class="s4">10</span><span class="s1">,</span>
                    <span class="s1">validation_data=test_dataset,</span>
                    <span class="s1">validation_steps=</span><span class="s4">30</span><span class="s1">)</span>

<span class="s1">test_loss, test_acc = model.evaluate(test_dataset)</span>

<span class="s1">print(</span><span class="s3">'Test Loss:'</span><span class="s1">, test_loss)</span>
<span class="s1">print(</span><span class="s3">'Test Accuracy:'</span><span class="s1">, test_acc)</span>

<span class="s0"># Predict on a sample text w/o padding.</span>
<span class="s1">sample_text = (</span><span class="s3">'The movie was not good. The animation and the graphics '</span>
               <span class="s3">'were terrible. I would not recommend this movie.'</span><span class="s1">)</span>
<span class="s1">predictions = model.predict(np.array([sample_text]))</span>
<span class="s1">print(predictions)</span>

<span class="s1">plt.figure(figsize=(</span><span class="s4">16</span><span class="s1">, </span><span class="s4">6</span><span class="s1">))</span>
<span class="s1">plt.subplot(</span><span class="s4">1</span><span class="s1">, </span><span class="s4">2</span><span class="s1">, </span><span class="s4">1</span><span class="s1">)</span>
<span class="s1">plot_graphs(history, </span><span class="s3">'accuracy'</span><span class="s1">)</span>
<span class="s1">plt.subplot(</span><span class="s4">1</span><span class="s1">, </span><span class="s4">2</span><span class="s1">, </span><span class="s4">2</span><span class="s1">)</span>
<span class="s1">plot_graphs(history, </span><span class="s3">'loss'</span><span class="s1">)</span>
</pre>
</body>
</html>